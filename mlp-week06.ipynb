{"cells":[{"cell_type":"markdown","source":"# Machine Learning in Python - Workshop 6","metadata":{"cell_id":"00000-553a5804-a245-413d-97a4-6dc0197900ef","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"# 1. Setup\n\n\n## 1.1 Packages\n","metadata":{"cell_id":"00001-fad26215-8b0b-4901-9a13-0e9fd7f61f7a","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"In the cell below we will load the core libraries we will be using for this workshop and setting some sensible defaults for our plot size and resolution. ","metadata":{"cell_id":"00002-1b8c4926-89df-442d-a982-64aa007dfe44","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00003-177280ef-8f7e-4deb-a875-c89b3b3ab495","deepnote_to_be_reexecuted":false,"source_hash":"ed6b4749","execution_millis":2654,"execution_start":1614160101419,"output_cleared":true,"deepnote_cell_type":"code"},"source":"# Display plots inline\n%matplotlib inline\n\n# Data libraries\nimport pandas as pd\nimport numpy as np\n\n# Plotting libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Plotting defaults\nplt.rcParams['figure.figsize'] = (10,6)\nplt.rcParams['figure.dpi'] = 80\n\n# ipython interactive widgets\nfrom ipywidgets import interact\n\n# sklearn modules\nimport sklearn\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV, KFold","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"--- \n\n# 2. Kernel Ridge Regression\n\nThis is an approach that combines Ridge regression with the kernel trick to allow for the fitting of more complex relationships. Beginning with the Ridge regression optimization formulation,\n\n$$ \\underset{\\boldsymbol{\\beta}}{\\text{argmin}} \\; \\lVert \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\beta} \\rVert^2 + \\alpha (\\boldsymbol{\\beta}^T\\boldsymbol{\\beta}) $$ \n\nWe have seen that it has a closed form solution where\n\n$$ \\boldsymbol{\\beta} = \\left( \\boldsymbol{X}\\boldsymbol{X}^\\top + \\alpha \\boldsymbol{I}_p \\right)^{-1} \\boldsymbol{X}^\\top \\boldsymbol{y} $$\n\nwhich can be rewritten as \n\n$$ \n\\begin{aligned}\n\\boldsymbol{\\beta} \n    &= \\boldsymbol{X}^\\top \\left( \\boldsymbol{X}^\\top\\boldsymbol{X} + \\alpha \\boldsymbol{I}_p \\right)^{-1}  \\boldsymbol{y} \\\\\n    &= \\boldsymbol{X}^\\top \\left( \\boldsymbol{K} + \\alpha \\boldsymbol{I}_p \\right)^{-1}  \\boldsymbol{y}\n\\end{aligned}\n$$\n\nwhere $\\boldsymbol{K}$ is a Gram matrix defined by a given kernel.\n\nThe implementation in scikit-learn is provided by the `KernelRidge` function in the `sklearn.kernel_ridge` submodule.","metadata":{"cell_id":"00004-61d663c1-7140-4802-867f-5237862d76e8","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00005-6602b51b-1689-47e3-b1fa-32af40be2115","deepnote_to_be_reexecuted":false,"source_hash":"3c414c81","execution_millis":95,"execution_start":1614160104075,"output_cleared":true,"deepnote_cell_type":"code"},"source":"from sklearn.kernel_ridge import KernelRidge","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will use this model to fit the gaussian process data we originally examined in week 3.","metadata":{"cell_id":"00006-90f0442d-d69f-41b7-a48d-5e02bee948bd","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00007-64bfbcc9-4467-4bc8-a219-8f1e93db7a29","deepnote_to_be_reexecuted":false,"source_hash":"ee5f9cd0","execution_millis":5,"execution_start":1614160104173,"output_cleared":true,"deepnote_cell_type":"code"},"source":"gp = pd.read_csv('gp.csv').sample(frac=1)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00008-03647e46-1af5-4b2d-bd78-16dd5ce576e3","deepnote_to_be_reexecuted":false,"source_hash":"1ed4b251","execution_millis":219,"execution_start":1614160104189,"output_cleared":true,"deepnote_cell_type":"code"},"source":"sns.scatterplot(x='x', y='y', data=gp)\nplt.show()","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00009-a4bc9ad9-cbbe-4858-9ee9-5e77d7000bcc","deepnote_to_be_reexecuted":false,"source_hash":"e6aac642","execution_millis":3,"execution_start":1614160104393,"output_cleared":true,"deepnote_cell_type":"code"},"source":"# Define outcome vector and model matrix\ny = gp.y\nX = gp.drop('y', axis=1)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.1 Model Parameters\n\nTo fit a kernel ridge regression model we first must choose a kernel function, here we will be using a radial basis function (`rbf`), other possible choices are available as part of the `sklearn.metrics.pairwise` submodule. Each kernel will have additional parameters, generally sklearn attempts to label all primary kernel parameters using the name `gamma` and any additional required parameters will have more informative names. In the lectures this parameter was referred to as the bandwidth and labelled as $l$. The parameterization of the radial basis function used by scikit-learn is as follows:\n\n$$\nK(\\boldsymbol{x}, \\boldsymbol{y}) = \\exp(-\\gamma \\lVert \\boldsymbol{x}-\\boldsymbol{y} \\rVert^2).\n$$\n\nAs such, when fitting a rbf based kernel ridge regression model we now have two parameters to optimize over: `alpha` from the ridge $\\ell_2$ penalty, and `gamma` the bandwidth of the rbf kernel.\n\nBelow we define a function that fits the kernel ridge regression to the `gp` data and then plots the fit along with reporting the root mean squared error. By including the `interact` decorator we provide simple interactive widgets that you can use to adjust these two parameters to explore their effect on the model's fit.","metadata":{"cell_id":"00010-dff63eb1-5c81-4e5f-9d3a-8bd04afc5460","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00011-c4b0832d-184e-4ff5-b23e-268b4f3de205","deepnote_to_be_reexecuted":false,"source_hash":"a7a85e93","execution_millis":1,"execution_start":1614160104400,"output_cleared":true,"deepnote_cell_type":"code"},"source":"def kernel_ridge_fit(alpha, gamma):\n    # Fit the model\n    m = KernelRidge(kernel='rbf', alpha=alpha, gamma=gamma).fit(X, y)\n\n    rmse = np.sqrt(mean_squared_error(y, m.predict(X)))\n\n    # Create DF for predictions\n    gp_pred = pd.DataFrame({'x': np.linspace(0, 1, num=100)})\n    gp_pred[\"y\"] = m.predict(gp_pred)\n\n    sns.scatterplot(x='x', y='y', data=gp)\n    sns.lineplot(x='x', y='y', data=gp_pred, color='black')\n    plt.title(\"alpha = %s gamma = %.1f\\nrmse = %.4f\" % (alpha, gamma, rmse))\n    plt.show()\n","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can try adjusting the values of these parameters in the code chunk below and exploring the effect they have on the overall model fit. We recommend trying values of `alpha` between 0 and 1 and `gamma` between 0 and 1000. Note the behavior of `gamma` will depend on your choice of `alpha`.","metadata":{"cell_id":"00012-bcf25672-3b69-4452-b076-ce4b30a5d221","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00013-e6ff45bb-7d0c-49d7-a976-c0aeddcdbe7c","deepnote_to_be_reexecuted":false,"source_hash":"c973edda","execution_millis":345,"execution_start":1614160104449,"output_cleared":true,"deepnote_cell_type":"code"},"source":"kernel_ridge_fit(alpha = 0.01, gamma = 100)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 1\n\nHow does changing the value of `alpha` and `gamma` affect the fit of this model? Which of the two seems more important? Explain.","metadata":{"cell_id":"00014-c23583b2-f423-4994-8a23-3ab350d2507e","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"cell_id":"00015-6c58dbf0-baa9-4b92-8f91-09b51c0892ef","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\n## 2.2 Model Tuning\n\nAs with ridge and lasso regression models we can use `GridSearchCV` to explore the space of possible parameters to determine the optimal model under cross validation. ","metadata":{"cell_id":"00016-299d47da-fa45-40eb-903b-03751238bb8c","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00017-04a456dc-7326-44e4-b53f-246b4098c6e1","deepnote_to_be_reexecuted":false,"source_hash":"977925ff","execution_start":1614160104799,"execution_millis":36871,"output_cleared":true,"deepnote_cell_type":"code"},"source":"m = GridSearchCV(\n    KernelRidge(kernel='rbf'),\n    param_grid={\"alpha\": np.logspace(-3, 0, num=4),\n                \"gamma\": np.logspace(0, 3, num=50)},\n    scoring = 'neg_mean_squared_error', \n    cv = KFold(5, shuffle=True, random_state=1234)\n)\n\nm = m.fit(X,y)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that with two parameters we now need to fit $n_\\alpha \\times n_\\gamma \\times n_{cv}$ models which can be somewhat slow. \n\nFor our grid search we obtain the following as the best values of these parameters:","metadata":{"cell_id":"00018-48777488-f697-44f9-ac64-f98b3d4e7bbe","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00019-c610cb90-2e1d-4a8f-b823-9eacce565dd6","deepnote_to_be_reexecuted":false,"source_hash":"88b045b3","execution_start":1614160141670,"execution_millis":6,"output_cleared":true,"deepnote_cell_type":"code"},"source":"m.best_params_","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once the parameter values have been determined we can then examine the model's performance with the entire data set using the `kernel_ridge_fit` function we previously defined.","metadata":{"cell_id":"00020-e526cd3d-4fde-4b69-ab42-aaa2635ba311","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00021-8b1020e5-2ecc-490f-983c-92466d4c56f6","deepnote_to_be_reexecuted":false,"source_hash":"e519aa42","execution_start":1614160141671,"execution_millis":395,"output_cleared":true,"deepnote_cell_type":"code"},"source":"kernel_ridge_fit(m.best_params_[\"alpha\"], m.best_params_[\"gamma\"])","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 2\n\nRepeat the grid search above but fix `alpha` to be 0.01, 0.1 and 1 and search over the possible values of `gamma`. For each of these three models compare the rmses of the models.","metadata":{"cell_id":"00022-aac47b53-c1f6-4cf3-ac50-e956939ca751","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00023-953ecb06-5861-4855-b79f-8c25a2a9c44c","deepnote_to_be_reexecuted":false,"source_hash":"e2d326fc","execution_start":1614160142045,"execution_millis":2,"output_cleared":true,"deepnote_cell_type":"code"},"source":"# alpha = 0.01","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00024-6d951b76-9e94-4482-a232-9eddaea75873","deepnote_to_be_reexecuted":false,"source_hash":"3fd58a23","execution_start":1614160142050,"execution_millis":1,"output_cleared":true,"deepnote_cell_type":"code"},"source":"# alpha = 0.1","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00025-e981d9b6-df55-4efd-b208-29dbb549884f","deepnote_to_be_reexecuted":false,"source_hash":"672c5b7a","execution_start":1614160142054,"execution_millis":0,"output_cleared":true,"deepnote_cell_type":"code"},"source":"# alpha = 1","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"cell_id":"00026-568eb7a3-727d-49f7-b71a-bd065b4a0acc","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 3\n\nBased on your finding in Exercise 2 comment on the relative importance of `alpha` for the performance of the model.","metadata":{"cell_id":"00027-6e511ea2-a474-4055-93e4-fd0c7f732ade","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"cell_id":"00028-f4b4f64b-b02e-416c-b990-3f8e5152f4be","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 4\n\nWhat happens if you set `alpha = 0`?","metadata":{"cell_id":"00029-b0eade11-9279-492e-860a-03a44eef2bcb","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00030-10731848-3385-4153-b7c2-68205262e257","deepnote_to_be_reexecuted":false,"source_hash":"b623e53d","execution_start":1614160142061,"execution_millis":0,"output_cleared":true,"deepnote_cell_type":"code"},"source":"","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"cell_id":"00031-fea223b1-b829-4d2a-800e-cd173a981339","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\n# 3. Logistic Regression (Introduction)\n \n## 3.1 Data\n\nFor this introduction we will be using a data set on spam emails from the [OpenIntro project](https://www.openintro.org/), these data are similar in spirit to those used in lecture (which came from Machine Learning: A Probabilistic Perspective). These data have been provided as `email.csv` along with this worksheet. A full data dictionary can be found [here](https://www.openintro.org/data/index.php?data=email). To keep things simple this week we will restrict our exploration to including only the following columns: `spam`, `exclaim_mess`, `format`, `num_char`,  `line_breaks`, and `number`. Brief descriptions of these variables are below:\n\n* `spam` - Indicator for whether the email was spam.\n* `exclaim_mess` - The number of exclamation points in the email message.\n* `format` - Indicates whether the email was written using HTML (e.g. may have included bolding or active links).\n* `num_char` - The number of characters in the email, in thousands.\n* `line_breaks` - The number of line breaks in the email (does not count text wrapping).\n* `number` - Factor variable saying whether there was no number, a small number (under 1 million), or a big number.","metadata":{"cell_id":"00032-74410366-cd9e-4815-afff-1ad6ea6cc1f2","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00033-5f8f9d36-39ef-423a-a86d-77e14436d927","deepnote_to_be_reexecuted":false,"source_hash":"67475667","execution_start":1614160142063,"execution_millis":67,"output_cleared":true,"deepnote_cell_type":"code"},"source":"email = pd.read_csv('email.csv')[ ['spam', 'exclaim_mess', 'format', 'num_char', 'line_breaks', 'number'] ]\nemail","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Given that our data includes the `number` column which is categorical, we will take care of the necessary dummy coding (specifically one hot encoding here) before we begin to explore and model these data. Here we will use the pandas' `get_dummies` function but a similar outcome can be achieved using sklearn's `OneHotEncoder`.","metadata":{"cell_id":"00034-9777bcf6-9ca2-49c9-b2c7-66b1b18f26ed","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00035-0824ecf1-ce96-48ba-b8c6-4bd1f8f03193","deepnote_to_be_reexecuted":false,"source_hash":"d98e4579","execution_start":1614160142126,"execution_millis":52,"output_cleared":true,"deepnote_cell_type":"code"},"source":"email = pd.get_dummies(email)\nemail","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n## 3.2 Exploration\n\nWe can construct a pair plot of these data to explore some of the basic relationships between the outcome and the features, since our outcome variable, `spam`, is binary (categorical) we can use it is as the `hue` argument with the `pairplot` function to get a better sense about how spam and non-spam messages differ across our features.","metadata":{"cell_id":"00036-4f321d6e-3b2d-444a-a13a-351710113dea","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00037-6592b1da-65db-4f17-9aba-476cf21fcd0a","deepnote_to_be_reexecuted":false,"source_hash":"6bbe1ef0","execution_start":1614160142174,"execution_millis":32036,"output_cleared":true,"deepnote_cell_type":"code"},"source":"sns.pairplot(email, hue='spam', diag_kind='hist')","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 5\n\nOf the plots provided by the pair plot which are the most useful? Explain.","metadata":{"cell_id":"00038-8a60c1cf-2fbf-49a2-8e32-1dd1c3a3c0ce","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"cell_id":"00039-daf44510-9092-4898-aea4-c00b8e8275d4","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\nWhen plotting data with categorical or binary features there are a number of additional plotting methods in seaborn that are useful for exploring potential relationships. In particular, `stripplot`, `swarmplot` and `violinplot` can all provide useful insights while avoiding the overplotting issue.\n\nAnother useful type of plot for data like this is known as a strip plot which adds jitter to avoid the overplotting while showing \"all\" of the data, which is particularly important when your categories are not balanced.","metadata":{"cell_id":"00040-11b252f1-4a9b-4332-b58d-18778cb8ffdf","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00041-efaa8a97-1795-41f8-bc1d-3d7b3f8ddd81","deepnote_to_be_reexecuted":false,"source_hash":"c38b32eb","execution_start":1614160174208,"output_cleared":true,"deepnote_cell_type":"code"},"source":"# Since some methods show all points, we will sample the data to get a more managable n\n# swarmplot in particular is very slow for large n\n\nemail_samp = email.sample(frac=0.1, random_state=1234)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00042-0ea7bc57-1eff-4bb4-bdb1-085b58690fb0","deepnote_to_be_reexecuted":false,"source_hash":"ba3cf684","execution_start":1614160174253,"execution_millis":569,"output_cleared":true,"deepnote_cell_type":"code"},"source":"plt.figure(figsize=(12,6))\n\nplt.subplot(131)\nsns.stripplot(y='num_char', x='spam', data=email_samp)\n\nplt.subplot(132)\nsns.swarmplot(y='num_char', x='spam', size=3, data=email_samp)\n\nplt.subplot(133)\nsns.violinplot(y='num_char', x='spam', data=email_samp)\n\nplt.show()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 6\n\nComment on any structure you see in these plots, does `num_char` appear to have different distributions between the spam and non-spam emails?","metadata":{"cell_id":"00043-fb3ad9a6-ff95-4bb5-bffa-d34f17cdaa8e","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"cell_id":"00044-4aa70249-eed3-4163-92e5-17bf8f742925","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\nSince the `num_char` variable is quite right skewed we can explore using a log transform of the variable before plotting,","metadata":{"cell_id":"00045-5d6b4dfa-75eb-4688-b147-f0e53f630e01","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00046-a96bd200-4752-4c4d-bca6-58a60771fed3","deepnote_to_be_reexecuted":false,"source_hash":"cc2b92fd","execution_start":1614160174826,"execution_millis":1,"output_cleared":true,"deepnote_cell_type":"code"},"source":"email_samp['log_num_char'] = np.log(email_samp.num_char)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00047-8c9b4d9c-e4d1-46a1-8b98-04c76085a5d5","deepnote_to_be_reexecuted":false,"source_hash":"c8c72f50","execution_start":1614160174831,"execution_millis":562,"output_cleared":true,"deepnote_cell_type":"code"},"source":"plt.figure(figsize=(12,6))\n\nplt.subplot(131)\nsns.stripplot(y='log_num_char', x='spam', data=email_samp)\n\nplt.subplot(132)\nsns.swarmplot(y='log_num_char', x='spam', size=4, data=email_samp)\n\nplt.subplot(133)\nsns.violinplot(y='log_num_char', x='spam', data=email_samp)\n\nplt.show()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 7\n\nComment on any structure you see in these plots, does `log_num_char` appear to have different distributions between the spam and non-spam emails?","metadata":{"cell_id":"00048-045ccf34-fe79-4630-a581-fa7ca7423291","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"cell_id":"00049-bac9016d-dee5-4867-8596-2c67372602bb","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\n## 3.2 Model Fitting\n\nLogistic (and multinomial) regression models are fit using the `LogisticRegression` model function from the `linear_model` submodule of sklearn. ","metadata":{"cell_id":"00050-c8e18e74-9e13-416e-9ad5-86cddf6de868","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00051-f7c8aa5f-51f2-49a2-b8d3-5a4f1cafa95d","deepnote_to_be_reexecuted":false,"source_hash":"af35732f","execution_start":1614160175384,"execution_millis":1,"output_cleared":true,"deepnote_cell_type":"code"},"source":"from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00052-57f337b9-9450-4f3e-8170-85fbb7ceff7e","deepnote_to_be_reexecuted":false,"source_hash":"2611693f","execution_start":1614160175389,"execution_millis":3,"output_cleared":true,"deepnote_cell_type":"code"},"source":"y = email.spam\nX = email.drop('spam', axis=1)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are a couple of \"unique\" properties of this model implementation in sklearn that are important to be aware of. The first is that the model can be fit with regularization (i.e. an $\\ell_2$ or $\\ell_1$ penalty) but the inclusion of the $\\ell_2$ penalty is the default behavior for these models. \n\nFor those coming from other programming / modeling languages this may be somewhat surprising, this strange default is probably the most common reason that results from sklearn might not immediately match results from other tools. This behavior can be explicitly controlled via the `penalty` argument. Note that if you did wish to include a penalty on the coefficients then just like with ridge or lasso it is necessary to tune this penalty parameter. However, `LogisticRegression` does not use `alpha` for this tuning parameter but instead uses `C` which is the inverse of the `alpha` we have been using - i.e. smaller values of `C` imply more regularization. \n\nOtherwise, this model function behaves as all of the other model functions we have seen so far, below we fit the model and then extract the fitted coefficients.","metadata":{"cell_id":"00053-a70c5893-8d20-496f-bd8c-94c90ca8c238","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00054-6833af60-3934-49bc-8bdc-e4b7b9d7ded0","deepnote_to_be_reexecuted":false,"source_hash":"3b259064","execution_start":1614160175399,"execution_millis":435,"output_cleared":true,"deepnote_cell_type":"code"},"source":"# Note we use `fit_intercept = False` to avoid a rank deficient model matrix sicne\n# we used one-hot encoding for the `number` feature.\n\nm = LogisticRegression(penalty = 'none', fit_intercept = False, solver='lbfgs', max_iter=1000).fit(X, y)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00055-591053e6-5f06-4434-843d-b0a2183a059c","deepnote_to_be_reexecuted":false,"source_hash":"e48a938","execution_start":1614160175838,"execution_millis":11,"output_cleared":true,"deepnote_cell_type":"code"},"source":"# This is just some fancy python code for attaching the column names to \n# their corresponding coefficeint values\n\ndict(zip(X.columns, zip(*m.coef_)))","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 8\n\nBased on these results, interpret the coefficient for `num_char` in context.","metadata":{"cell_id":"00056-440befcf-4157-42a5-aa18-ceb00b9f3f39","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"cell_id":"00057-f8dc1147-1299-4a14-b737-8283199d7807","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 9\n\nGenerally comment on how each feature is related to the probability of an email being spam.","metadata":{"cell_id":"00058-f0b5dfb7-c5c4-4714-8b3f-b3cd6671c9c3","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"cell_id":"00059-4cde088e-7946-49f2-8f84-6a76d507b4d8","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\n## 3.3 Model Predictions\n\nUnlike with our previous regression models, the fitted `LogisticRegression` objects provide multiple prediction methods. Specifically: `predict` which predicts the class label (either `0` or `1`), `predict_proba` which predicts the class probabilities, and `predict_log_proba` which predicts the log probabilities of each class.","metadata":{"cell_id":"00060-c63b1da7-b503-48da-9dd3-22f8d745b51e","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00061-e1d065a7-3f92-4158-a642-6881ab2269cf","deepnote_to_be_reexecuted":false,"source_hash":"8f813c9b","execution_start":1614160175852,"execution_millis":71,"output_cleared":true,"deepnote_cell_type":"code"},"source":"m.predict(X.head())","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00062-e5a43fba-9f6e-4174-8b52-7d412a8a3c67","deepnote_to_be_reexecuted":false,"source_hash":"9868c304","execution_start":1614160175925,"execution_millis":22,"output_cleared":true,"deepnote_cell_type":"code"},"source":"m.predict_proba(X.head())","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00063-2c82c4f3-ee3e-4199-8571-3fa55267fc01","deepnote_to_be_reexecuted":false,"source_hash":"877371c8","execution_start":1614160175944,"execution_millis":14,"output_cleared":true,"deepnote_cell_type":"code"},"source":"m.predict_log_proba(X.head())","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here the class label prediction is based on which of the classes has the largest probability. In this case the model is predicting that all five of these messages are not spam (`spam==0`).","metadata":{"cell_id":"00064-091eac43-4b25-4135-a603-c67c93909ba4","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 10\n\nAccording to this model, what is the probability that a message with 5 exclamation marks, unformated, with 5000 characters, 10 line breaks, and no numbers was spam?","metadata":{"cell_id":"00065-e697a4cf-eb49-4fec-ab23-864c860afdba","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00066-201ceed8-0f0e-4665-b701-fd19e4372baa","deepnote_to_be_reexecuted":false,"source_hash":"b623e53d","execution_start":1614160175997,"execution_millis":1,"output_cleared":true,"deepnote_cell_type":"code"},"source":"","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n## 3.3 Model Validation\n\n### 3.3.1 Confusion Matrix\n\nAs our outcome variable of interest is binary, either 0 or 1, using metrics like root mean squared error does not make sense. Instead we use metrics that measure our classifier's ability to correctly label our data using the following possible outcomes for each prediction:\n\n* *true positives* - labeled as spam when it is spam\n* *false positives* - labeled as spam when it is not spam\n* *true negatives* - labeled as not spam when it is not spam\n* *false negatives* - labeled as not spam when it is spam\n\nThe `predict` function chooses these labels based on whichever probability is larger, in the binary case this is equivalent to asking if the probability of being spam is $\\geq 0.5$ for each observation. This choice of threshold however is arbitrary and we can use any value between 0 and 1 for determining what we will label spam vs not spam. Below we engage is some data book keeping and then define the `confusion_plot` function for visualizing these different outcomes for the different possible threshold values. ","metadata":{"cell_id":"00067-e2f97154-1c82-40a9-8469-ffafd7d070f8","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00068-39178552-e234-47b0-8b56-064840547464","deepnote_to_be_reexecuted":false,"source_hash":"891f38bc","execution_start":1614160175998,"execution_millis":1,"output_cleared":true,"deepnote_cell_type":"code"},"source":"# This transformation is necessary so that seaborn behaves correctly when plotting the data horizontally\ntruth = pd.Categorical.from_codes(y, categories = ('not spam','spam'))\nprobs = m.predict_proba(X)[:,1]","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00069-3fdea004-f5b1-4f25-8c93-4960e2fa997e","deepnote_to_be_reexecuted":false,"source_hash":"4471f62d","execution_start":1614160176022,"execution_millis":2,"output_cleared":true,"deepnote_cell_type":"code"},"source":"def confusion_plot(threshold=0.5):\n    d = pd.DataFrame(\n        data = {'spam': y, 'truth': truth, 'probs': probs}\n    )\n    \n    # Create a column called outcome that contains the labeling outcome\n    # for the given threshold\n    d['outcome'] = 'other'\n    d.loc[(d.spam == 1) & (d.probs >= threshold), 'outcome'] = 'true positive'\n    d.loc[(d.spam == 0) & (d.probs >= threshold), 'outcome'] = 'false positive'\n    d.loc[(d.spam == 1) & (d.probs <  threshold), 'outcome'] = 'false negative'\n    d.loc[(d.spam == 0) & (d.probs <  threshold), 'outcome'] = 'true negative'\n    \n    # Create plot and color according to outcome\n    plt.figure(figsize=(12,4))\n    plt.xlim((-0.05,1.05))\n    sns.stripplot(y='truth', x='probs', hue='outcome', data=d)\n    plt.axvline(x=threshold, linestyle='dashed', color='black', alpha=0.5)\n    plt.title(\"threshold = %.2f\" % threshold)\n    plt.show()\n    \n    return sklearn.metrics.confusion_matrix(y_true=d.spam, y_pred=d.probs >= threshold)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below we run this function using a threshold of 0.5 to examine how the logistic regression classifier is performing. Feel free to vary this between 0 and 1 and observe the effect.","metadata":{"cell_id":"00070-2f7356e4-8551-4871-b25f-514f5e855ed0","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00071-fd83d295-f821-47e8-bef5-64b57aca290c","deepnote_to_be_reexecuted":false,"source_hash":"5245eee","execution_start":1614160176073,"execution_millis":412,"output_cleared":true,"deepnote_cell_type":"code"},"source":"confusion_plot(threshold=0.5)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The small matrix that is returned by the function is the confusion matrix for the given threshold and it contains the number of each outcome (in the same order as presented in the graph), this is calculated use the `confusion_matrix` function from the `sklearn.metrics` submodule.","metadata":{"cell_id":"00072-f141d04a-31b0-4eaf-9d5f-d7798bddd77d","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 11\n\nBased on these results what value of threshold for this model might be a reasonable choice? Think about the relative cost of a false negative vs a false positive.","metadata":{"cell_id":"00073-19c4f5c1-9873-4254-8dac-59a5d8c55b95","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"cell_id":"00074-3754c255-f715-4ec3-b03e-6189f5d52529","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\n### 3.3.2 ROC curves\n\nAnother method for seeing the effect of these different choices of threshold is to generate a Receiver operating characteristic (ROC) curve for the model which plots the true positive rate against the false positive rate for different threshold values.","metadata":{"cell_id":"00075-5798f012-7e98-474f-83ab-809efeb548c7","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00076-beddb1f0-9d37-4bc8-886d-f4ed3ecb8c8c","deepnote_to_be_reexecuted":false,"source_hash":"a3c54144","execution_start":1614160176477,"execution_millis":2,"output_cleared":true,"deepnote_cell_type":"code"},"source":"from sklearn.metrics import roc_curve, precision_recall_curve","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This function returns a tuple of the false positive rate, true positive rate, and threshold values based on the true labels and the predicted probabilities. \n\nHere we take those results and construct a data frame to enable us to create the ROC plot using seaborn. ","metadata":{"cell_id":"00077-04d8c4d9-1a5d-42da-833c-d5f27d100d2e","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00079-6c8e0a05-505b-4c38-8bd9-fbbb5b4e4b72","deepnote_to_be_reexecuted":false,"source_hash":"2bf033fb","execution_start":1614160176483,"execution_millis":39,"output_cleared":true,"deepnote_cell_type":"code"},"source":"roc_calc = roc_curve(y_true=y, y_score=probs)\n\nroc = pd.DataFrame(\n    data = np.c_[roc_calc],\n    columns = ('false positive rate', 'true positive rate', 'threshold')\n)\n\nroc","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using this data frame, the function below creates the ROC curve (blue) via a seaborn lineplot, while the remaining code draws the point on the ROC curve that corresponds to the given threshold value (red), and finally the 0-1 line (grey, dashed). The 0-1 line is the ROC curve expected for a model that determines labels by chance (e.g. flipping a coin with `heads` is spam, `tails` is not spam as a model). ","metadata":{"cell_id":"00080-0d15ea64-2d38-482e-8ac5-52962e1b90ee","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00081-3a06387c-cd74-4a50-bf3c-9364cba0cb10","deepnote_to_be_reexecuted":false,"source_hash":"796c62a0","execution_start":1614160176518,"execution_millis":1,"output_cleared":true,"deepnote_cell_type":"code"},"source":"def roc_plot(threshold=0.5):\n    i = (np.abs(roc.threshold - threshold)).idxmin()\n\n    sns.lineplot(x='false positive rate', y='true positive rate', data=roc, ci=None)\n\n    plt.plot([0,1],[0,1], 'k--', alpha=0.5) # 0-1 line \n    plt.plot(roc.iloc[i,0], roc.iloc[i,1], 'r.')\n\n    plt.title(\"threshold = %.2f\" % threshold)\n    plt.show()","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00082-73446663-148c-41d3-925c-a33ce5e6acca","deepnote_to_be_reexecuted":false,"source_hash":"68e60215","execution_start":1614160176525,"execution_millis":195,"output_cleared":true,"deepnote_cell_type":"code"},"source":"roc_plot(threshold=0.5)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 12\n\nTry adjusting the threshold value in `roc_plot`, what happens to the red point as this value is changed?","metadata":{"cell_id":"00083-00adf5bd-ba39-4331-80c7-616f79f4b59d","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"cell_id":"00084-ffb2cfa9-46ab-461d-9548-f029e7b858dc","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\n### 3.3.3 Area under the curve (AUC)\n\nFinally, a common summary statistic that is used to summarize the performance of a binary classifier over all the possible thresholds is the area under the curve (AUC), which as the name implies, is the area under the ROC curve. This can be calculated from the prediction results using the `roc_auc_score` function  from the `sklearn.metrics` submodule.","metadata":{"cell_id":"00085-8d21a8e8-be12-4ad7-8d6f-3a860393dbc6","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00086-1de56a88-5cf2-4118-9672-50cb504fc576","deepnote_to_be_reexecuted":false,"source_hash":"5e810c6e","execution_start":1614160176717,"execution_millis":9,"output_cleared":true,"deepnote_cell_type":"code"},"source":"sklearn.metrics.roc_auc_score(y_true=y, y_score=probs)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Beyond giving a simple numeric summary statistic for a classifier, the AUC also has a useful alternative interpretation - the AUC also equals the probability that the classifier will rank a randomly chosen positive case (`spam` here) higher than a randomly chosen negative case (`not spam` here).","metadata":{"cell_id":"00087-dc2fd003-5670-4b5b-ad36-81c97123f7a7","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 13\n\nBased on these and the preceeding results, how well does this classifier perform? Justify your answer.","metadata":{"cell_id":"00088-b30a57d4-ad03-49a4-afbe-4a48a8db673f","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"cell_id":"00089-8b64092f-ed8b-45d8-93da-ce651e1c94bc","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\nNote that the AUC does not tell us what value of threshold should be used for our classifier, just its overall performance. If we would like to choose an optimal threshold value for practical decision making then we need to define a specific loss function for our data / problem. Specifically, we need to have some numeric measure of the benefit for true positives and true negatives and the costs of false negatives and false positives. For example the cost of a false positive vs. false negative is very different for this spam example than it would be for a medical diagnostic test like a Cancer screening test.","metadata":{"cell_id":"00090-8cc38d63-b4ba-4883-8f6c-c9d635d07453","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\n## 4. Competing the worksheet\n\nAt this point you have hopefully been able to complete all the preceeding exercises. Now \nis a good time to check the reproducibility of this document by restarting the notebook's\nkernel and rerunning all cells in order.\n\nOnce that is done and you are happy with everything, you can then run the following cell \nto generate your PDF and turn it in on gradescope under the `mlp-week06` assignment.","metadata":{"tags":[],"cell_id":"00091-8788453e-d509-47c7-aca7-073e5746f0e6","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"deepnote_to_be_reexecuted":false,"execution_millis":14040,"output_cleared":true,"source_hash":"3248841c","tags":[],"cell_id":"00092-8ca6702d-91a0-4422-9b3f-f0731c75956e","execution_start":1614160176765,"deepnote_cell_type":"code"},"source":"!jupyter nbconvert --to pdf mlp-week06.ipynb","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=87e92c81-516a-42e6-8e83-d8875e4d6ff5' target=\"_blank\">\n<img style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":4,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.2"},"deepnote_notebook_id":"3d673876-b81e-42ce-808e-7df32080b25c","deepnote":{},"deepnote_execution_queue":[]}}